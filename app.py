import streamlit as st
import os
from dotenv import load_dotenv
from io import BytesIO
import pandas as pd
import json
import tiktoken
from zipfile import ZipFile

# Importando as fun√ß√µes dos outros arquivos
from relatorio import get_token, get_workspaces_id, scan_workspace, clean_reports, upload_file
from documenta import generate_docx, generate_excel, text_to_document, Documenta, defined_prompt_fontes, defined_prompt_medidas, generate_promt_medidas, generate_promt_fontes, defined_prompt, generate_promt

# Importando o sistema de internacionaliza√ß√£o
from i18n import init_i18n, t, language_selector

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Initialize internationalization
init_i18n(default_language="en")

MODELO = ""
MAX_TOKENS = 0
MAX_TOKENS_SAIDA = 0

def counttokens(text):
    # Inicializando o tokenizador para o modelo desejado (neste exemplo, GPT-4)
    encoding = tiktoken.get_encoding("cl100k_base")
    
    # Contar o n√∫mero de tokens no texto fornecido
    tokens = len(encoding.encode(text))
    
    return tokens

def configure_app():
    """Configura a apar√™ncia e o layout do aplicativo Streamlit."""    
    st.set_page_config(
        page_title="AutoDoc",
        page_icon="üìä",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    
    # Language selector and GitHub button at the top
    col1, col2, col3, col4 = st.columns([1, 1.5, 1, 0.7])
    with col3:
        language_selector("main_language_selector", use_flags=True, flag_style="image")
    with col4:
        # CSS para fazer o bot√£o GitHub parecer com os bot√µes do Streamlit
        st.markdown("""
        <style>
        /* Estilo para o bot√£o GitHub simular bot√µes do Streamlit */
        .github-button {
            display: inline-block;
            width: 65%;
            padding: 0.375rem 0.5rem;
            margin: 0;
            font-weight: 400;
            color: #262730;
            background-color: #FFFFFF;
            border: 1px solid #D4D4D8;
            border-radius: 0.375rem;
            text-decoration: none !important;
            text-align: center;
            font-size: 0.875rem;
            line-height: 1.5;
            cursor: pointer;
            transition: all 0.15s ease;
            min-height: 38.4px;
            box-sizing: border-box;
        }
        .github-button:hover {
            border-color: #FF4B4B;
            color: #FF4B4B;
            text-decoration: none !important;
        }
        .github-button:visited {
            text-decoration: none !important;
        }
        .github-button:active {
            text-decoration: none !important;
        }
        .github-button:focus {
            text-decoration: none !important;
            outline: none;
        }
        .github-button svg {
            vertical-align: middle;
            margin-right: 6px;
        }
        
        /* Para tema escuro */
        @media (prefers-color-scheme: dark) {
            .github-button {
                color: #FAFAFA;
                background-color: #262730;
                border-color: #4A4A4A;
            }
            .github-button:hover {
                border-color: #FF4B4B;
                color: #FF4B4B;
                text-decoration: none !important;
            }
        }
        </style>
        """, unsafe_allow_html=True)
        
        # Bot√£o GitHub com estilo similar aos bot√µes das bandeiras
        st.markdown(f"""
        <a href="https://github.com/LawrenceTeixeira/PBIAutoDoc" target="_blank" class="github-button" aria-label="{t('ui.github_aria_label')}">
            <svg width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
            </svg>
            {t('ui.github_button_text')}
        </a>
        """, unsafe_allow_html=True)
    
    st.header(t('ui.app_title'))
    st.write(t('ui.app_description'))

def sidebar_inputs():
    """Exibe o menu lateral para inser√ß√£o das informa√ß√µes do administrador e upload do arquivo template do Power BI."""    
    with st.sidebar:
        
        st.image("https://lawrence.eti.br/wp-content/uploads/2025/04/AutoDoc.png")
        
        # Carrega os modelos das vari√°veis de ambiente
        available_models_str = os.getenv('AVAILABLE_MODELS', 'groq/meta-llama/llama-4-maverick-17b-128e-instruct,groq/openai/gpt-oss-120b,gpt-5-mini,gpt-5,azure/gpt-4.1-mini,gemini/gemini-2.5-flash-preview-04-17,claude-3-7-sonnet-20250219,deepseek/deepseek-chat')
        available_models = tuple(model.strip() for model in available_models_str.split(','))
        default_model = os.getenv('DEFAULT_MODEL', available_models[0])
        
        # Encontra o √≠ndice do modelo padr√£o para definir como selecionado
        try:
            default_index = available_models.index(default_model)
        except ValueError:
            default_index = 0
        
        # Op√ß√£o de sele√ß√£o de modelo usando vari√°veis de ambiente
        modelo = st.selectbox(
            t('ui.model_selector'), 
            available_models,
            index=default_index
        )
                         
        # Op√ß√£o de sele√ß√£o entre Servi√ßo e Arquivo
        option = st.radio(t('ui.data_source_selector'), (t('ui.power_bi_template'), t('ui.power_bi_service')))
        
        if option == t('ui.power_bi_template'):
            app_id = None
            tenant_id = None
            secret_value = None
            uploaded_files = st.file_uploader(
                t('ui.file_upload_label'), 
                accept_multiple_files=True, 
                type=['pbit', 'zip'], 
                help=t('ui.file_upload_help')
            )
        else:
            app_id = st.text_input(
                label=t('ui.app_id_label'),
                help=t('ui.app_id_help')
            )
            tenant_id = st.text_input(
                label=t('ui.tenant_id_label'),
                help=t('ui.tenant_id_help')
            )
            secret_value = st.text_input(
                label=t('ui.secret_value_label'),
                help=t('ui.secret_value_help'),
                type='password'
            )
            uploaded_files = None  # Nenhum arquivo ser√° necess√°rio            

        # Set a slider to select max tokens
        max_tokens = st.sidebar.number_input(t('ui.max_tokens_input'), min_value=256, max_value=10000000, value=8192, step=256)

        # Set a slider to select max tokens
        max_tokens_saida = st.sidebar.number_input(t('ui.max_tokens_output'), min_value=512, max_value=128000, value=8192, step=512)             
        
        ""
        
        st.sidebar.markdown(t('ui.created_by', author="[Lawrence Teixeira](https://www.linkedin.com/in/lawrenceteixeira/)"))

    return app_id, tenant_id, secret_value, uploaded_files, modelo, max_tokens, max_tokens_saida

def detailed_description():
    """Mostra uma explica√ß√£o detalhada sobre o aplicativo."""    
    st.write(f"""
    {t('detailed_description.title')}
    {t('detailed_description.features')}
    
    - {t('detailed_description.feature_1')}
    - {t('detailed_description.feature_2')}
    - {t('detailed_description.feature_3')}
    - {t('detailed_description.feature_4')}

    {t('detailed_description.purpose')}

    {t('detailed_description.how_to_use')}
    {t('detailed_description.step_1')}
    {t('detailed_description.step_2')}
    {t('detailed_description.step_3')}

    {t('detailed_description.conclusion')}
    
    {t('detailed_description.created_info')}
    
    Vers√£o: 2.0.0 
    """)

def sidebar_description():
    """Mostra uma descri√ß√£o resumida com bot√£o para mais informa√ß√µes na barra lateral."""    
    st.sidebar.header(t('ui.about_app'))
    if st.sidebar.button(t('ui.information')):
        st.session_state.show_description = not st.session_state.get('show_description', False)
        
    if st.session_state.get('show_description', False):
        detailed_description()
                
def main_content(headers=None, uploaded_files=None):
    """Exibe as informa√ß√µes principais do aplicativo."""    
    if 'all_reports_data' not in st.session_state:
        st.session_state['all_reports_data'] = []

    if uploaded_files:
        # Convert to list if needed
        if not isinstance(uploaded_files, list):
            uploaded_files = [uploaded_files]
        
        # Check if single file or batch processing
        if len(uploaded_files) == 1:
            # Single file processing - original behavior with all options
            with st.spinner(t('messages.processing_file')):
                try:
                    result = upload_file(uploaded_files[0])
                    
                    # Verificar se o resultado √© uma string (erro) ou tupla (sucesso)
                    if isinstance(result, str):
                        # √â uma mensagem de erro
                        error_message = result
                        if "Arquivo inv√°lido: n√£o cont√©m 'DataModelSchema'" in error_message or "n√£o √© um ZIP/PBIT v√°lido" in error_message:
                            st.error(t('errors.invalid_powerbi_file'))
                        elif "Arquivo n√£o suportado" in error_message:
                            st.error(t('errors.file_not_supported'))
                        elif "arquivo vazio" in error_message.lower():
                            st.error(t('errors.empty_file'))
                        elif "corrompido" in error_message.lower() or "falha" in error_message.lower():
                            st.error(t('errors.corrupted_file'))
                        else:
                            st.error(t('errors.processing_error', error=error_message))
                        return
                    
                    # Se chegou aqui, deve ser uma tupla com os dados
                    if isinstance(result, tuple) and len(result) == 2:
                        df_normalized, df_relationships = result
                    else:
                        st.error(t('errors.invalid_powerbi_file'))
                        return
                    
                except ValueError as e:
                    if "too many values to unpack" in str(e):
                        st.error(t('errors.invalid_powerbi_file'))
                    else:
                        st.error(t('errors.processing_error', error=str(e)))
                    return
                except Exception as e:
                    st.error(t('errors.processing_error', error=str(e)))
                    return

            # Store the df_relationships data in the session state for later use
            st.session_state['df_relationships'] = df_relationships

            if isinstance(df_normalized, pd.DataFrame) and not df_normalized.empty:
                st.success(t('messages.file_processed'))
                buttons_download(df_normalized)
            else:
                st.error(t('errors.no_data_found'))
        else:
            # Multiple files - batch processing without extra options
            st.session_state['all_reports_data'] = []
            errors = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, uploaded_file in enumerate(uploaded_files):
                status_text.text(f"{t('messages.processing_file')} {idx + 1}/{len(uploaded_files)}: {uploaded_file.name}")
                progress_bar.progress((idx) / len(uploaded_files))
                
                try:
                    result = upload_file(uploaded_file)
                    
                    # Verificar se o resultado √© uma string (erro) ou tupla (sucesso)
                    if isinstance(result, str):
                        error_message = result
                        if "Arquivo inv√°lido: n√£o cont√©m 'DataModelSchema'" in error_message or "n√£o √© um ZIP/PBIT v√°lido" in error_message:
                            errors.append(f"{uploaded_file.name}: {t('errors.invalid_powerbi_file')}")
                        elif "Arquivo n√£o suportado" in error_message:
                            errors.append(f"{uploaded_file.name}: {t('errors.file_not_supported')}")
                        elif "arquivo vazio" in error_message.lower():
                            errors.append(f"{uploaded_file.name}: {t('errors.empty_file')}")
                        elif "corrompido" in error_message.lower() or "falha" in error_message.lower():
                            errors.append(f"{uploaded_file.name}: {t('errors.corrupted_file')}")
                        else:
                            errors.append(f"{uploaded_file.name}: {t('errors.processing_error', error=error_message)}")
                        continue
                    
                    # Se chegou aqui, deve ser uma tupla com os dados
                    if isinstance(result, tuple) and len(result) == 2:
                        df_normalized, df_relationships = result
                        if isinstance(df_normalized, pd.DataFrame) and not df_normalized.empty:
                            st.session_state['all_reports_data'].append({
                                'df': df_normalized,
                                'df_relationships': df_relationships,
                                'filename': uploaded_file.name.rsplit('.', 1)[0]
                            })
                        else:
                            errors.append(f"{uploaded_file.name}: {t('errors.no_data_found')}")
                    else:
                        errors.append(f"{uploaded_file.name}: {t('errors.invalid_powerbi_file')}")
                    
                except ValueError as e:
                    if "too many values to unpack" in str(e):
                        errors.append(f"{uploaded_file.name}: {t('errors.invalid_powerbi_file')}")
                    else:
                        errors.append(f"{uploaded_file.name}: {t('errors.processing_error', error=str(e))}")
                except Exception as e:
                    errors.append(f"{uploaded_file.name}: {t('errors.processing_error', error=str(e))}")
            
            progress_bar.progress(1.0)
            status_text.empty()
            progress_bar.empty()
            
            # Show results
            if st.session_state['all_reports_data']:
                st.success(f"{t('messages.files_processed_successfully', count=len(st.session_state['all_reports_data']))}")
                buttons_download_batch(st.session_state['all_reports_data'])
            
            if errors:
                st.error(f"{t('messages.processing_errors')}")
                for error in errors:
                    st.warning(error)

    if headers:        
        with st.spinner(t('messages.loading_workspaces')):
            workspace_dict = get_workspaces_id(headers)
        
        if workspace_dict:
            option = st.selectbox(
                t('ui.workspace_selector'), 
                list(workspace_dict.keys()), 
                index=None, 
                placeholder=t('ui.workspace_placeholder')
            )
            if option:
                with st.spinner(t('messages.scanning_workspace')):
                    workspace_id = workspace_dict[option]
                    scan_response = scan_workspace(headers, workspace_id)
                    display_reports(scan_response)

def display_reports(scan_response):
    """Exibe os pain√©is e lida com a sele√ß√£o do usu√°rio."""    
    report_names = [report_info['name'] for report_info in scan_response['datasets'] if 'PbixInImportMode' in report_info['contentProviderType'] and 'Usage Metrics Report' not in report_info['name']]
    
    option = st.selectbox(
        t('ui.report_selector'), 
        list(report_names), 
        index=None, 
        placeholder=t('ui.report_placeholder')
    )
    
    if option:
        df_desnormalized = clean_reports(scan_response, option)
        buttons_download(df_desnormalized)

def click_button():
    st.session_state.button = not st.session_state.button
    
# Function to recursively update the 'FonteDados' field
def update_fonte_dados(data, tables_df):
    if isinstance(data, dict):
        # Collect keys to modify in a separate list
        keys_to_update = []
        for key, value in data.items():
            if key == 'NomeTabela' and value in tables_df['NomeTabela'].to_list():
                keys_to_update.append((key, value))
            elif isinstance(value, (dict, list)):
                update_fonte_dados(value, tables_df)
        
        # Apply the modifications
        for key, value in keys_to_update:
            table_index = tables_df[tables_df['NomeTabela'] == value].index[0]
            data['FonteDados'] = tables_df['FonteDados'].iloc[table_index]
            
    elif isinstance(data, list):
        for item in data:
            update_fonte_dados(item, tables_df)  

def buttons_download(df):
    """Exibe bot√µes para download e visualiza√ß√£o dos dados processados."""    
    if not df.empty and 'ReportName' in df.columns:
        report_name = df['ReportName'].iloc[0].replace(' ', '_')
    else:
        report_name = "PBIReport"

    if 'button' not in st.session_state:
        st.session_state.button = True
    if 'show_chat' not in st.session_state:
        st.session_state.show_chat = False
    if 'doc_gerada' not in st.session_state:
        st.session_state['doc_gerada'] = False

    on = st.checkbox(t('ui.view_report_data'))
    if on:
        st.dataframe(df)

    verprompt_completo = st.checkbox(t('ui.show_prompt'))
    if verprompt_completo:
        document_text_all, dados_relatorio_PBI_medidas, dados_relatorio_PBI_fontes, measures_df, tables_df, df_colunas = text_to_document(df, max_tokens=MAX_TOKENS)
        prompt = generate_promt(document_text_all, t('language_name'))
        st.text_area("Prompt:", value=prompt, height=300)

    mostra_total_tokens = st.checkbox(t('ui.show_tokens'))
    if mostra_total_tokens:
        document_text_all, dados_relatorio_PBI_medidas, dados_relatorio_PBI_fontes, measures_df, tables_df, df_colunas = text_to_document(df, max_tokens=MAX_TOKENS)
        total_tokens = 0
        stringmostra = ""
        conta_interacao= 0
        if counttokens(document_text_all) < MAX_TOKENS:
            conta_interacao += 1
            total_tokens += counttokens(document_text_all)
            stringmostra += f"{t('ui.first_interaction')}      | {t('ui.tokens_count')} {counttokens(document_text_all):,}\n"
        else:
            for text in dados_relatorio_PBI_medidas:
                conta_interacao += 1
                total_tokens += counttokens(text)
                stringmostra += f"{conta_interacao}{t('ui.measures_interaction')}      | {t('ui.tokens_count')} {counttokens(text):,}\n"
            for text in dados_relatorio_PBI_fontes:
                conta_interacao += 1
                total_tokens += counttokens(text)
                stringmostra += f"{conta_interacao}{t('ui.sources_interaction')} | {t('ui.tokens_count')} {counttokens(text):,}\n"
        stringmostra += f"\n{t('ui.total_interactions')} {conta_interacao}\n{t('ui.total_tokens')} {total_tokens:,} tokens.\n"
        st.text_area(t('ui.token_analysis_label'), value=stringmostra, height=300)

    colA, colB = st.columns(2)
    with colA:
        gerar_doc = st.button(t('ui.generate_doc'), disabled=st.session_state.get('show_chat', False))
    with colB:
        conversar = st.button(t('ui.chat'), disabled=st.session_state.get('show_chat', False))

    if gerar_doc and not st.session_state.get('show_chat', False):
        conta_interacao = 1
        gerando = t('messages.generating_documentation')
        with st.spinner(gerando):
            document_text_all, dados_relatorio_PBI_medidas, dados_relatorio_PBI_fontes, measures_df, tables_df, df_colunas = text_to_document(df, max_tokens=MAX_TOKENS)
            medidas_do_relatorio_df = pd.DataFrame()
            fontes_de_dados_df = pd.DataFrame()
            Uma = True
            response_info = {}
            response_tables = []
            if counttokens(document_text_all) < MAX_TOKENS:
                response = Documenta(defined_prompt(t('language_name')), document_text_all, MODELO, max_tokens=MAX_TOKENS, max_tokens_saida=MAX_TOKENS_SAIDA)
                conta_interacao += 1
                if Uma and 'Relatorio' in response and 'Tabelas_do_Relatorio' in response:
                    Uma = False
                    response_info = response['Relatorio']
                    response_tables = response['Tabelas_do_Relatorio']
                response_measures = response['Medidas_do_Relatorio']
                response_source = response['Fontes_de_Dados']
            else:
                for text in dados_relatorio_PBI_medidas:
                    gerando = f"{conta_interacao}{t('ui.interaction_progress')}"
                    with st.spinner(gerando):
                        response = Documenta(defined_prompt_medidas(t('language_name')), text, MODELO, max_tokens=MAX_TOKENS, max_tokens_saida=MAX_TOKENS_SAIDA)
                        conta_interacao += 1
                        if Uma and 'Relatorio' in response and 'Tabelas_do_Relatorio' in response:
                            Uma = False
                            response_info = response['Relatorio']
                            response_tables = response['Tabelas_do_Relatorio']
                        if 'Medidas_do_Relatorio'  in response:
                            medidas_do_relatorio_df = pd.concat([medidas_do_relatorio_df, pd.DataFrame(response["Medidas_do_Relatorio"])], ignore_index=True)
                for text in dados_relatorio_PBI_fontes:
                    gerando = f"{conta_interacao}{t('ui.interaction_progress')}"
                    with st.spinner(gerando):
                        response = Documenta(defined_prompt_fontes(t('language_name')), text, MODELO, max_tokens=MAX_TOKENS, max_tokens_saida=MAX_TOKENS_SAIDA)
                        conta_interacao += 1
                        if Uma and 'Relatorio' in response and 'Tabelas_do_Relatorio' in response:
                            print(response)
                            Uma = False
                            response_info = response['Relatorio']
                            response_tables = response['Tabelas_do_Relatorio']
                        if 'Fontes_de_Dados' in response:
                            fontes_de_dados_df = pd.concat([fontes_de_dados_df, pd.DataFrame(response["Fontes_de_Dados"])], ignore_index=True)
                response_measures = medidas_do_relatorio_df.to_dict(orient='records')
                response_source = fontes_de_dados_df.to_dict(orient='records')
            
            update_fonte_dados(response_source, tables_df)
            
            st.session_state['response_info'] = response_info
            st.session_state['response_tables'] = response_tables
            st.session_state['response_measures'] = response_measures
            st.session_state['response_source'] = response_source
            st.session_state['measures_df'] = measures_df
            st.session_state['df_colunas'] = df_colunas
            st.session_state.button = False
            st.session_state['doc_gerada'] = True  # <-- Seta flag ap√≥s gerar documenta√ß√£o
            st.session_state['modelo'] = MODELO
            st.session_state.show_chat = False
            st.success(t('messages.documentation_generated'))

    if conversar and not st.session_state.get('show_chat', False):
        st.session_state.show_chat = True
        st.session_state['doc_gerada'] = False  # <-- Oculta op√ß√µes ao entrar no chat

    if st.session_state.show_chat:
        # --- Chat interface ---
        # Prepare chat prompt from the report
        document_text_all, _, _, _, _, _ = text_to_document(df, max_tokens=MAX_TOKENS)        # Adiciona colunas ao contexto
        df_colunas = st.session_state.get('df_colunas')
        if df_colunas is not None and not df_colunas.empty:
            colunas_texto = '\n'.join([
                f"Tabela: {row['NomeTabela']} | Coluna: {row['NomeColuna']} | Tipo: {row['TipoDadoColuna']} | TipoColuna: {row['TipoColuna']} | Express√£o: {row['ExpressaoColuna']}"
                for _, row in df_colunas.iterrows()
            ])
        else:
            colunas_texto = t('chat.no_columns_found')

        # Adiciona relacionamentos ao contexto
        df_relationships = st.session_state.get('df_relationships')
        if df_relationships is not None and not df_relationships.empty:
            relacionamentos_texto = '\n'.join([
                f"De: {row['FromTable']}.{row['FromColumn']} -> Para: {row['ToTable']}.{row['ToColumn']}"
                for _, row in df_relationships.iterrows()
            ])
        else:
            relacionamentos_texto = t('chat.no_relationships_found')

        chat_prompt = t('chat.system_prompt', document_text_all=document_text_all, colunas_texto=colunas_texto, relacionamentos_texto=relacionamentos_texto)
        if 'chat_messages' not in st.session_state:
            st.session_state['chat_messages'] = [
                {"role": "system", "content": chat_prompt + t('chat.system_instruction')},
                {"role": "assistant", "content": t('chat.assistant_greeting', report_name=report_name)}
            ]
        st.markdown("<hr>", unsafe_allow_html=True)
        st.subheader(t('chat.title'))
        for msg in st.session_state['chat_messages']:
            if msg["role"] != "system":
                st.chat_message(msg["role"]).write(msg["content"])
        user_input = st.chat_input(t('chat.input_placeholder'))
        if user_input:
            st.session_state['chat_messages'].append({"role": "user", "content": user_input})
            st.chat_message("user").write(user_input)
            with st.spinner(t('chat.processing')):
                from litellm import completion
                try:
                    response = completion(
                        model=MODELO,
                        temperature=0,
                        max_tokens=MAX_TOKENS_SAIDA,
                        messages=st.session_state['chat_messages']
                    )
                    result = response.choices[0].message.content
                except Exception as e:
                    result = t('chat.error', error=str(e))
            msg = {"role": "assistant", "content": result}
            st.session_state['chat_messages'].append(msg)
            st.chat_message("assistant").write(result)

        st.button(t('chat.close_chat'), on_click=lambda: st.session_state.update({'show_chat': False, 'doc_gerada': True}))    # Exibe as op√ß√µes somente se a documenta√ß√£o foi gerada e o chat n√£o est√° ativo
    if st.session_state.get('doc_gerada', False) and not st.session_state.get('show_chat', False):
        #verprompt = st.checkbox(t('ui.show_json'), key='mostrar_json', disabled=st.session_state.button )
        #if verprompt:
        #    response_info_str = json.dumps(st.session_state.get('response_info', {}), indent=2)
        #    response_tables_str = json.dumps(st.session_state.get('response_tables', {}), indent=2)
        #    response_measures_str = json.dumps(st.session_state.get('response_measures', {}), indent=2)
        #    response_source_str = json.dumps(st.session_state.get('response_source', {}), indent=2)
        #    text = f"{t('ui.json_report_info')}\n{response_info_str}\n\n{t('ui.json_report_tables')}\n{response_tables_str}\n\n{t('ui.json_report_measures')}\n{response_measures_str}\n\n{t('ui.json_data_sources')}\n{response_source_str}"
        #    st.text_area(t('ui.json_area_label'), value=text, height=300)

        col1, col2 = st.columns(2)
        with col1:
            if st.button(t('ui.export_excel'), disabled=st.session_state.button):
                with st.spinner(t('ui.generating_file')):
                    buffer = generate_excel(st.session_state['response_info'], st.session_state['response_tables'], st.session_state['response_measures'], st.session_state['response_source'], st.session_state['measures_df'], st.session_state['df_relationships'], st.session_state['df_colunas'])
                    st.download_button(
                        label=t('ui.download_excel_file'),
                        data=buffer,
                        file_name=report_name+'.xlsx',
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
        with col2:
            if st.button(t('ui.export_word'), disabled=st.session_state.button):
                with st.spinner(t('ui.generating_file')):
                    doc = generate_docx(st.session_state['response_info'], st.session_state['response_tables'], st.session_state['response_measures'], st.session_state['response_source'], st.session_state['measures_df'], st.session_state['df_relationships'], st.session_state['df_colunas'], st.session_state['modelo'], st.session_state.language)
                    buffer = BytesIO()
                    doc.save(buffer)
                    buffer.seek(0)
                    st.download_button(
                        label=t('ui.download_word_file'),
                        data=buffer,
                        file_name=report_name+'.docx',
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )

def buttons_download_batch(all_reports_data):
    """Exibe bot√µes para processamento e download em lote de m√∫ltiplos relat√≥rios."""
    if 'batch_button' not in st.session_state:
        st.session_state.batch_button = True
    if 'batch_doc_gerada' not in st.session_state:
        st.session_state['batch_doc_gerada'] = False
    if 'batch_results' not in st.session_state:
        st.session_state['batch_results'] = []

    st.subheader(f"{t('ui.batch_processing_title', count=len(all_reports_data))}")
    
    # Show preview of files to process
    with st.expander(t('ui.view_files_to_process')):
        for idx, report_data in enumerate(all_reports_data):
            st.write(f"{idx + 1}. {report_data['filename']}")
    
    # Options before generating documentation
    on = st.checkbox(t('ui.view_report_data'))
    if on:
        for idx, report_data in enumerate(all_reports_data):
            st.write(f"**{report_data['filename']}**")
            st.dataframe(report_data['df'])
    
    verprompt_completo = st.checkbox(t('ui.show_prompt'))
    if verprompt_completo:
        for report_data in all_reports_data:
            st.write(f"**{report_data['filename']}**")
            document_text_all, _, _, _, _, _ = text_to_document(report_data['df'], max_tokens=MAX_TOKENS)
            prompt = generate_promt(document_text_all, t('language_name'))
            st.text_area(f"Prompt - {report_data['filename']}:", value=prompt, height=300, key=f"prompt_{report_data['filename']}")
    
    mostra_total_tokens = st.checkbox(t('ui.show_tokens'))
    if mostra_total_tokens:
        for report_data in all_reports_data:
            st.write(f"**{report_data['filename']}**")
            document_text_all, dados_relatorio_PBI_medidas, dados_relatorio_PBI_fontes, _, _, _ = text_to_document(report_data['df'], max_tokens=MAX_TOKENS)
            total_tokens = 0
            stringmostra = ""
            conta_interacao = 0
            if counttokens(document_text_all) < MAX_TOKENS:
                conta_interacao += 1
                total_tokens += counttokens(document_text_all)
                stringmostra += f"{t('ui.first_interaction')}      | {t('ui.tokens_count')} {counttokens(document_text_all):,}\n"
            else:
                for text in dados_relatorio_PBI_medidas:
                    conta_interacao += 1
                    total_tokens += counttokens(text)
                    stringmostra += f"{conta_interacao}{t('ui.measures_interaction')}      | {t('ui.tokens_count')} {counttokens(text):,}\n"
                for text in dados_relatorio_PBI_fontes:
                    conta_interacao += 1
                    total_tokens += counttokens(text)
                    stringmostra += f"{conta_interacao}{t('ui.sources_interaction')} | {t('ui.tokens_count')} {counttokens(text):,}\n"
            stringmostra += f"\n{t('ui.total_interactions')} {conta_interacao}\n{t('ui.total_tokens')} {total_tokens:,} tokens.\n"
            st.text_area(f"{t('ui.token_analysis_label')} - {report_data['filename']}", value=stringmostra, height=200, key=f"tokens_{report_data['filename']}")
    
    gerar_batch = st.button(t('ui.generate_batch_docs'), disabled=st.session_state.get('batch_doc_gerada', False))
    
    if gerar_batch:
        st.session_state['batch_results'] = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, report_data in enumerate(all_reports_data):
            df = report_data['df']
            df_relationships = report_data['df_relationships']
            filename = report_data['filename']
            
            status_text.text(f"{t('messages.processing_report')} {idx + 1}/{len(all_reports_data)}: {filename}")
            progress_bar.progress(idx / len(all_reports_data))
            
            try:
                conta_interacao = 1
                document_text_all, dados_relatorio_PBI_medidas, dados_relatorio_PBI_fontes, measures_df, tables_df, df_colunas = text_to_document(df, max_tokens=MAX_TOKENS)
                medidas_do_relatorio_df = pd.DataFrame()
                fontes_de_dados_df = pd.DataFrame()
                Uma = True
                response_info = {}
                response_tables = []
                
                if counttokens(document_text_all) < MAX_TOKENS:
                    response = Documenta(defined_prompt(t('language_name')), document_text_all, MODELO, max_tokens=MAX_TOKENS, max_tokens_saida=MAX_TOKENS_SAIDA)
                    conta_interacao += 1
                    if Uma and 'Relatorio' in response and 'Tabelas_do_Relatorio' in response:
                        Uma = False
                        response_info = response['Relatorio']
                        response_tables = response['Tabelas_do_Relatorio']
                    response_measures = response['Medidas_do_Relatorio']
                    response_source = response['Fontes_de_Dados']
                else:
                    for text in dados_relatorio_PBI_medidas:
                        response = Documenta(defined_prompt_medidas(t('language_name')), text, MODELO, max_tokens=MAX_TOKENS, max_tokens_saida=MAX_TOKENS_SAIDA)
                        conta_interacao += 1
                        if Uma and 'Relatorio' in response and 'Tabelas_do_Relatorio' in response:
                            Uma = False
                            response_info = response['Relatorio']
                            response_tables = response['Tabelas_do_Relatorio']
                        if 'Medidas_do_Relatorio' in response:
                            medidas_do_relatorio_df = pd.concat([medidas_do_relatorio_df, pd.DataFrame(response["Medidas_do_Relatorio"])], ignore_index=True)
                    for text in dados_relatorio_PBI_fontes:
                        response = Documenta(defined_prompt_fontes(t('language_name')), text, MODELO, max_tokens=MAX_TOKENS, max_tokens_saida=MAX_TOKENS_SAIDA)
                        conta_interacao += 1
                        if Uma and 'Relatorio' in response and 'Tabelas_do_Relatorio' in response:
                            Uma = False
                            response_info = response['Relatorio']
                            response_tables = response['Tabelas_do_Relatorio']
                        if 'Fontes_de_Dados' in response:
                            fontes_de_dados_df = pd.concat([fontes_de_dados_df, pd.DataFrame(response["Fontes_de_Dados"])], ignore_index=True)
                    response_measures = medidas_do_relatorio_df.to_dict(orient='records')
                    response_source = fontes_de_dados_df.to_dict(orient='records')
                
                update_fonte_dados(response_source, tables_df)
                
                st.session_state['batch_results'].append({
                    'filename': filename,
                    'response_info': response_info,
                    'response_tables': response_tables,
                    'response_measures': response_measures,
                    'response_source': response_source,
                    'measures_df': measures_df,
                    'df_relationships': df_relationships,
                    'df_colunas': df_colunas
                })
            except Exception as e:
                st.error(f"{t('errors.processing_error', error=str(e))} - {filename}")
        
        progress_bar.progress(1.0)
        status_text.empty()
        progress_bar.empty()
        
        st.session_state['batch_doc_gerada'] = True
        st.session_state.batch_button = False
        st.success(t('messages.batch_documentation_generated', count=len(st.session_state['batch_results'])))
    
    # Display download options after generation
    if st.session_state.get('batch_doc_gerada', False):
        # Show JSON option
        #verprompt = st.checkbox(t('ui.show_json'), key='mostrar_json_batch', disabled=st.session_state.batch_button)
        #if verprompt:
        #    for result in st.session_state['batch_results']:
        #        st.write(f"**{result['filename']}**")
        #        response_info_str = json.dumps(result['response_info'], indent=2)
        #        response_tables_str = json.dumps(result['response_tables'], indent=2)
        #        response_measures_str = json.dumps(result['response_measures'], indent=2)
        #        response_source_str = json.dumps(result['response_source'], indent=2)
        #        text = f"{t('ui.json_report_info')}\n{response_info_str}\n\n{t('ui.json_report_tables')}\n{response_tables_str}\n\n{t('ui.json_report_measures')}\n{response_measures_str}\n\n{t('ui.json_data_sources')}\n{response_source_str}"
        #        st.text_area(f"{t('ui.json_area_label')} - {result['filename']}", value=text, height=300, key=f"json_{result['filename']}")
        
        st.subheader(t('ui.download_batch_title'))
        
        # Download all files as ZIP
        col_zip1, col_zip2 = st.columns(2)
        with col_zip1:
            if st.button(t('ui.download_all_excel_zip'), disabled=st.session_state.batch_button, key='download_all_excel_zip'):
                with st.spinner(t('ui.generating_files')):
                    zip_buffer = BytesIO()
                    with ZipFile(zip_buffer, 'w') as zip_file:
                        for result in st.session_state['batch_results']:
                            excel_buffer = generate_excel(
                                result['response_info'],
                                result['response_tables'],
                                result['response_measures'],
                                result['response_source'],
                                result['measures_df'],
                                result['df_relationships'],
                                result['df_colunas']
                            )
                            zip_file.writestr(f"{result['filename']}.xlsx", excel_buffer.getvalue())
                    zip_buffer.seek(0)
                    st.download_button(
                        label=t('ui.download_zip_excel'),
                        data=zip_buffer,
                        file_name="PowerBI_Reports_Excel.zip",
                        mime="application/zip",
                        key="download_excel_zip_btn"
                    )
        
        with col_zip2:
            if st.button(t('ui.download_all_word_zip'), disabled=st.session_state.batch_button, key='download_all_word_zip'):
                with st.spinner(t('ui.generating_files')):
                    zip_buffer = BytesIO()
                    with ZipFile(zip_buffer, 'w') as zip_file:
                        for result in st.session_state['batch_results']:
                            doc = generate_docx(
                                result['response_info'],
                                result['response_tables'],
                                result['response_measures'],
                                result['response_source'],
                                result['measures_df'],
                                result['df_relationships'],
                                result['df_colunas'],
                                MODELO,
                                st.session_state.language
                            )
                            doc_buffer = BytesIO()
                            doc.save(doc_buffer)
                            doc_buffer.seek(0)
                            zip_file.writestr(f"{result['filename']}.docx", doc_buffer.getvalue())
                    zip_buffer.seek(0)
                    st.download_button(
                        label=t('ui.download_zip_word'),
                        data=zip_buffer,
                        file_name="PowerBI_Reports_Word.zip",
                        mime="application/zip",
                        key="download_word_zip_btn"
                    )
        
        st.markdown("---")
        st.write(t('ui.individual_downloads'))
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button(t('ui.export_all_excel'), disabled=st.session_state.batch_button):
                with st.spinner(t('ui.generating_files')):
                    for result in st.session_state['batch_results']:
                        buffer = generate_excel(
                            result['response_info'],
                            result['response_tables'],
                            result['response_measures'],
                            result['response_source'],
                            result['measures_df'],
                            result['df_relationships'],
                            result['df_colunas']
                        )
                        st.download_button(
                            label=f"üì• {result['filename']}.xlsx",
                            data=buffer,
                            file_name=f"{result['filename']}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            key=f"excel_{result['filename']}"
                        )
        
        with col2:
            if st.button(t('ui.export_all_word'), disabled=st.session_state.batch_button):
                with st.spinner(t('ui.generating_files')):
                    for result in st.session_state['batch_results']:
                        doc = generate_docx(
                            result['response_info'],
                            result['response_tables'],
                            result['response_measures'],
                            result['response_source'],
                            result['measures_df'],
                            result['df_relationships'],
                            result['df_colunas'],
                            MODELO,
                            st.session_state.language
                        )
                        buffer = BytesIO()
                        doc.save(buffer)
                        buffer.seek(0)
                        st.download_button(
                            label=f"üì• {result['filename']}.docx",
                            data=buffer,
                            file_name=f"{result['filename']}.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            key=f"word_{result['filename']}"
                        )

        
def main():    
    """Fun√ß√£o principal do aplicativo, onde todas as fun√ß√µes s√£o chamadas."""        
    configure_app()
            
    global API_KEY, MODELO, MAX_TOKENS, MAX_TOKENS_SAIDA

    app_id, tenant_id, secret_value, uploaded_files, modelo, max_tokens, max_tokens_saida = sidebar_inputs()
    
    MODELO = modelo
    MAX_TOKENS = max_tokens
    MAX_TOKENS_SAIDA = max_tokens_saida
            
    if app_id and tenant_id and secret_value:
        headers = get_token(app_id, tenant_id, secret_value)
        if headers:
            main_content(headers, None)
    
    if uploaded_files:
        main_content(None, uploaded_files)

    if 'show_description' not in st.session_state:
        st.session_state.show_description = False

    sidebar_description()



if __name__ == "__main__":
    main()